{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0c713d-60f7-456b-a71a-0b5c1097cacc",
   "metadata": {},
   "source": [
    "# ML Metadata\n",
    "Taken from deeplearning.ai & tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a61b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-data-validation\n",
      "  Obtaining dependency information for tensorflow-data-validation from https://files.pythonhosted.org/packages/6e/ea/610f13945b479757dd411bfbba3efd4bcb02d1605f55b47017fdc749c63b/tensorflow_data_validation-1.14.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_data_validation-1.14.0-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-data-validation) (1.4.0)\n",
      "Collecting apache-beam[gcp]<3,>=2.47 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for apache-beam[gcp]<3,>=2.47 from https://files.pythonhosted.org/packages/fd/8b/fd13e1d2898bb2823a8da680ef9ceae590b7774c96c4bf676692e4e7e757/apache_beam-2.54.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached apache_beam-2.54.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-data-validation) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-data-validation) (1.23.5)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-data-validation) (1.5.3)\n",
      "Collecting protobuf<5,>=3.20.3 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for protobuf<5,>=3.20.3 from https://files.pythonhosted.org/packages/ad/6e/1bed3b7c904cc178cb8ee8dbaf72934964452b3de95b7a63412591edb93c/protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: pyarrow<11,>=10 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-data-validation) (10.0.1)\n",
      "Requirement already satisfied: pyfarmhash<0.4,>=0.2.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-data-validation) (0.3.2)\n",
      "Requirement already satisfied: six<2,>=1.12 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-data-validation) (1.16.0)\n",
      "Collecting tensorflow<3,>=2.13 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorflow<3,>=2.13 from https://files.pythonhosted.org/packages/7c/3c/049400232ee2897d613db9009e42269417166c3f8519d46e5c97b6f9e206/tensorflow-2.15.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.15.0-cp310-cp310-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-metadata<1.15,>=1.14.0 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorflow-metadata<1.15,>=1.14.0 from https://files.pythonhosted.org/packages/41/23/3705c7139886c079ef4c0e3be56a5a1fb90e9ee413a4b7caaee0ee0ea6fe/tensorflow_metadata-1.14.0-py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tfx-bsl<1.15,>=1.14.0 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for tfx-bsl<1.15,>=1.14.0 from https://files.pythonhosted.org/packages/9b/37/ef68d7d870d75fd335c66eb60a93f6181f4c5af498fec8c8b7170e3271ad/tfx_bsl-1.14.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached tfx_bsl-1.14.0-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.7)\n",
      "Requirement already satisfied: orjson<4,>=3.9.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.9.10)\n",
      "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.2.1)\n",
      "Collecting fastavro<2,>=0.23.6 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for fastavro<2,>=0.23.6 from https://files.pythonhosted.org/packages/21/2c/413f8edb84b499855c38ddb43659c953efdae62051b54c0571323f51b89f/fastavro-1.9.4-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached fastavro-1.9.4-cp310-cp310-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for fasteners<1.0,>=0.3 from https://files.pythonhosted.org/packages/61/bf/fd60001b3abc5222d8eaa4a204cd8c0ae78e75adc688f33ce4bf25b7fafa/fasteners-0.19-py3-none-any.whl.metadata\n",
      "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.54.2)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Using cached hdfs-2.7.3-py3-none-any.whl\n",
      "Collecting httplib2<0.23.0,>=0.8 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for httplib2<0.23.0,>=0.8 from https://files.pythonhosted.org/packages/a8/6c/d2fbdaaa5959339d53ba38e94c123e4e84b8fbc4b84beb0e70d7c1608486/httplib2-0.22.0-py3-none-any.whl.metadata\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting js2py<1,>=0.74 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Using cached Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.17.3)\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for jsonpickle<4.0.0,>=3.0.0 from https://files.pythonhosted.org/packages/d3/25/6e0a450430b7aa194b0f515f64820fc619314faa289458b7dfca4a026114/jsonpickle-3.0.2-py3-none-any.whl.metadata\n",
      "  Using cached jsonpickle-3.0.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for objsize<0.8.0,>=0.6.1 from https://files.pythonhosted.org/packages/98/26/00ba2cd7d79935ecefa384020828f3a96c3c9c9b66faf9d93aa16eb75985/objsize-0.7.0-py3-none-any.whl.metadata\n",
      "  Using cached objsize-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting packaging>=22.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for packaging>=22.0 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.6.0)\n",
      "Collecting proto-plus<2,>=1.7.1 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for proto-plus<2,>=1.7.1 from https://files.pythonhosted.org/packages/ad/41/7361075f3a31dcd05a6a38cfd807a6eecbfb6dbfe420d922cd400fc03ac1/proto_plus-1.23.0-py3-none-any.whl.metadata\n",
      "  Using cached proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2022.6)\n",
      "Requirement already satisfied: regex>=2020.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2023.8.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.9.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.22.0)\n",
      "Requirement already satisfied: pyarrow-hotfix<1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.6)\n",
      "Requirement already satisfied: cachetools<6,>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (5.2.0)\n",
      "Collecting google-api-core<3,>=2.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-api-core<3,>=2.0.0 from https://files.pythonhosted.org/packages/0f/87/373ab788a4682adc1a6900e54d54c750b7bd4be456d75b8bf64eccc23ef9/google_api_core-2.17.1-py3-none-any.whl.metadata\n",
      "  Using cached google_api_core-2.17.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-apitools<0.5.32,>=0.5.31 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Using cached google_apitools-0.5.31-py3-none-any.whl\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.15.0)\n",
      "Collecting google-auth-httplib2<0.2.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-auth-httplib2<0.2.0,>=0.1.0 from https://files.pythonhosted.org/packages/d3/3d/e4991229886c0d522d9552151a43ff7adcc61e026e60ce8bd508387f84cf/google_auth_httplib2-0.1.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth_httplib2-0.1.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting google-cloud-datastore<3,>=2.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-datastore<3,>=2.0.0 from https://files.pythonhosted.org/packages/a3/45/bd74899a0ed936118b60661c0bd983279e5d9c7307996e03b9501fc1a78b/google_cloud_datastore-2.19.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_datastore-2.19.0-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-cloud-pubsub<3,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-pubsub<3,>=2.1.0 from https://files.pythonhosted.org/packages/0e/34/9e8ee83376d2e92311022c2925b252039dad18a5eb6c2a08f56cf3e3e85d/google_cloud_pubsub-2.19.4-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_pubsub-2.19.4-py2.py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-pubsublite<2,>=1.2.0 from https://files.pythonhosted.org/packages/7e/16/3e6b0168a365a816365867d4bffac27239fd960e905f308395d39372df68/google_cloud_pubsublite-1.9.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_pubsublite-1.9.0-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-cloud-storage<3,>=2.14.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-storage<3,>=2.14.0 from https://files.pythonhosted.org/packages/3d/48/574463fbf30c7021341ab0620e56103a8c49ad864bdd177935306c057986/google_cloud_storage-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_storage-2.14.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting google-cloud-bigquery<4,>=2.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-bigquery<4,>=2.0.0 from https://files.pythonhosted.org/packages/49/dc/df04020251473cacbb0001bc8c1c065ea46d6ce4d4dd218b3bfa246bdb5a/google_cloud_bigquery-3.17.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_bigquery-3.17.2-py2.py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting google-cloud-bigquery-storage<3,>=2.6.3 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-bigquery-storage<3,>=2.6.3 from https://files.pythonhosted.org/packages/75/93/a4192dd34b42ab31c8411810db896deca31c48f845807a733602ac38d849/google_cloud_bigquery_storage-2.24.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_bigquery_storage-2.24.0-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-cloud-core<3,>=2.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-core<3,>=2.0.0 from https://files.pythonhosted.org/packages/5e/0f/2e2061e3fbcb9d535d5da3f58cc8de4947df1786fe6a1355960feb05a681/google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-bigtable<3,>=2.19.0 from https://files.pythonhosted.org/packages/c3/fe/7964847709481fb8f716810b2ab9d4c246466cf09d04e930b8abf6ee05f2/google_cloud_bigtable-2.23.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_bigtable-2.23.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-spanner<4,>=3.0.0 from https://files.pythonhosted.org/packages/90/42/da76a1a31eb041e721dabd30bddf0c7fbed2dca5d567bf59c5750fb13974/google_cloud_spanner-3.42.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_spanner-3.42.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-dlp<4,>=3.0.0 from https://files.pythonhosted.org/packages/a9/b6/6840336cdc1e69a174b93e237307f6f1224f5efe1ded5c68406c773ea9dd/google_cloud_dlp-3.15.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_dlp-3.15.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting google-cloud-language<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-language<3,>=2.0 from https://files.pythonhosted.org/packages/32/49/750ca8c7ea38e51085aa2d952238a8daef5ce09a548b9e13b5e9cf44f5af/google_cloud_language-2.13.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_language-2.13.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-videointelligence<3,>=2.0 from https://files.pythonhosted.org/packages/3d/47/198da672766fe5eb8f4f5ef46c18855359849f578fb47dc10c8bc1b5ac46/google_cloud_videointelligence-2.13.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_videointelligence-2.13.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-vision<4,>=2 from https://files.pythonhosted.org/packages/c3/b8/ed7d94b3268372b374e25b7b83debbae9b3f162b38cf2b1ddffef3a5269f/google_cloud_vision-3.7.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_vision-3.7.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-recommendations-ai<0.11.0,>=0.1.0 from https://files.pythonhosted.org/packages/55/c0/d7f22427b193a85f3d8cc3c551ba4e272e9a761d96f0a864643a54488dea/google_cloud_recommendations_ai-0.10.8-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_recommendations_ai-0.10.8-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting google-cloud-aiplatform<2.0,>=1.26.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-aiplatform<2.0,>=1.26.0 from https://files.pythonhosted.org/packages/ad/39/e3efba085187293d60a0dfb993c7804b480ba62b115ea1fd0c4bb6c4f469/google_cloud_aiplatform-1.42.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_aiplatform-1.42.1-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Collecting tensorflow-intel==2.15.0 (from tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorflow-intel==2.15.0 from https://files.pythonhosted.org/packages/da/1b/63e20bde0db52c3be7e078b50cf507f4534ad6e47b5e2b01d9ed63bb652f/tensorflow_intel-2.15.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.15.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (3.8.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (16.0.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (63.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (2.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (0.31.0)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorboard<2.16,>=2.15 from https://files.pythonhosted.org/packages/37/12/f6e9b9dcc310263cbd3948274e286538bd6800fd0c268850788f14a0c6d0/tensorboard-2.15.2-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (2.15.0)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for keras<2.16,>=2.15.0 from https://files.pythonhosted.org/packages/fc/a7/0d4490de967a67f68a538cc9cdb259bff971c4b5787f7765dc7c8f118f71/keras-2.15.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-metadata<1.15,>=1.14.0->tensorflow-data-validation) (1.59.1)\n",
      "Collecting protobuf<5,>=3.20.3 (from tensorflow-data-validation)\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "Collecting google-api-python-client<2,>=1.7.11 (from tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation)\n",
      "  Using cached google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Collecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorflow-serving-api<3,>=2.13.0 from https://files.pythonhosted.org/packages/3c/4d/ce445324599c1a999ec9b78ad22f391ed91a4785522a7628ff8c99527a1d/tensorflow_serving_api-2.14.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_serving_api-2.14.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation) (3.0.1)\n",
      "Collecting oauth2client>=1.4.12 (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.9)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-resource-manager<3.0.0dev,>=1.3.3 from https://files.pythonhosted.org/packages/17/10/3388aed8323a303644e69df696f23fe870e1519106a8786e03d502bdd7f1/google_cloud_resource_manager-1.12.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_resource_manager-1.12.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.0.3)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0 (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-resumable-media<3.0dev,>=0.6.0 from https://files.pythonhosted.org/packages/b2/c6/1202ef64a9336d846f713107dac1c7a0b016cb3840ca3d5615c7005a23d1/google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for grpc-google-iam-v1<1.0.0dev,>=0.12.4 from https://files.pythonhosted.org/packages/66/a0/d27ec874fb0a86b3609b73161a15cf633924888afa05c1673b3ab5a6c3f4/grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: grpcio-status>=1.33.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.54.2)\n",
      "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (7.3.1)\n",
      "Requirement already satisfied: sqlparse>=0.4.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.4.4)\n",
      "Requirement already satisfied: deprecated>=1.2.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.2.14)\n",
      "Collecting grpc-interceptor>=0.15.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for grpc-interceptor>=0.15.4 from https://files.pythonhosted.org/packages/15/ac/8d53f230a7443401ce81791ec50a3b0e54924bf615ad287654fa4a2f5cdc/grpc_interceptor-0.15.4-py3-none-any.whl.metadata\n",
      "  Using cached grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting google-auth<3,>=1.18.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-auth<3,>=1.18.0 from https://files.pythonhosted.org/packages/ff/ce/1b4dc8b5ecdc9a99202b093729192b69301c33064d0e312fb8d9e384dbe0/google_auth-2.28.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.28.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3,>=2.14.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Using cached google_crc32c-1.5.0-cp310-cp310-win_amd64.whl (27 kB)\n",
      "Requirement already satisfied: docopt in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.0.9)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from js2py<1,>=0.74->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (5.2)\n",
      "Requirement already satisfied: pyjsparser>=2.5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from js2py<1,>=0.74->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.19.3)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2022.9.24)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (0.40.0)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status>=1.33.2 (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/eb/97/e7dfe2d5566bca05f52af5d4f4a67ccb90878586d3cadbdf8de5a5d4be00/grpcio_status-1.60.1-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.60.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/d9/bd/f46d6511088f314cfedc880721fd32d387b8513b22da01cf4771d7439a2b/grpcio_status-1.60.0-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.60.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/0f/31/9f87b4d6a5a03c92bab47d54bf516b7196667441e86550280178714bdb28/grpcio_status-1.59.3-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.59.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/ea/4b/d31e321be28de4ea7d2dc00e0ec4b5c47696207bed4ced1b82b5146aa76c/grpcio_status-1.59.2-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.59.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/55/ce/e6d0382610240439ced22fe2183bcc387946bf80e5e0f17f5b5250978ff3/grpcio_status-1.59.0-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.59.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/f6/48/2bcf11bc2df159564eac099ea38d80663d291a56fa2f2f561d08bf083dfa/grpcio_status-1.58.0-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.58.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/d0/3f/347d93056572fdbd64d4f0fc58a18d420763a7118f8b177437d9dab0ae6f/grpcio_status-1.57.0-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/ef/16/3018689d96918e9c4c7407adf96b721df4d6748ba65db82c5eaa63564335/grpcio_status-1.56.2-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.56.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/2b/21/aaff30111c5941fd9adb5abbf06e04a0e491a685f48ffb291f72ad595ec7/grpcio_status-1.56.0-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.56.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/b6/c8/0efd0cf1ff62b3acc28619b9ba80a6ead4eb0ee42bb1c3b3841610af98a7/grpcio_status-1.55.3-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.55.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/b9/ad/51cf59eb26ffa5360ba9dc318580b777e5dda1aa344ad34475103d97e5ac/grpcio_status-1.54.3-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.54.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/d0/b5/ce78c4c0ebe6943a79d669b2971bd6ebaebae8dde3ca81814436ab00a583/grpcio_status-1.54.0-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.54.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/fb/c0/8ee53675cb1aecaa7fcbc9bea6575cb8c19a21dd0ce80fe6e5edb04426f4/grpcio_status-1.53.2-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.53.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/61/14/6a4476403abaf954c0b5715de9cfdb7528143c5ac372316fa95704ae8551/grpcio_status-1.53.1-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.53.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/8e/a7/54f2404c17bbed0af2e60028b5310fd195ec0693a98098766668fa69e44e/grpcio_status-1.53.0-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.53.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/cf/06/93caa3804c8b129772662812925a055ae47c56b037e6554cac91a50094de/grpcio_status-1.51.3-py3-none-any.whl.metadata\n",
      "  Using cached grpcio_status-1.51.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Using cached grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "  Using cached grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.4.8)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (0.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (2.2.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tzlocal>=1.2->js2py<1,>=0.74->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2023.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (2.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow<3,>=2.13->tensorflow-data-validation) (3.2.2)\n",
      "Using cached tensorflow_data_validation-1.14.0-cp310-cp310-win_amd64.whl (1.3 MB)\n",
      "Using cached tensorflow-2.15.0-cp310-cp310-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.15.0-cp310-cp310-win_amd64.whl (300.9 MB)\n",
      "Using cached tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Using cached tfx_bsl-1.14.0-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "Using cached fastavro-1.9.4-cp310-cp310-win_amd64.whl (497 kB)\n",
      "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Using cached google_api_core-2.17.1-py3-none-any.whl (137 kB)\n",
      "Using cached google_auth_httplib2-0.1.1-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached google_cloud_aiplatform-1.42.1-py2.py3-none-any.whl (3.5 MB)\n",
      "Using cached google_cloud_bigquery-3.17.2-py2.py3-none-any.whl (230 kB)\n",
      "Using cached google_cloud_bigquery_storage-2.24.0-py2.py3-none-any.whl (190 kB)\n",
      "Using cached google_cloud_bigtable-2.23.0-py2.py3-none-any.whl (357 kB)\n",
      "Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_cloud_datastore-2.19.0-py2.py3-none-any.whl (176 kB)\n",
      "Using cached google_cloud_dlp-3.15.1-py2.py3-none-any.whl (159 kB)\n",
      "Using cached google_cloud_language-2.13.1-py2.py3-none-any.whl (143 kB)\n",
      "Using cached google_cloud_pubsub-2.19.4-py2.py3-none-any.whl (273 kB)\n",
      "Using cached google_cloud_pubsublite-1.9.0-py2.py3-none-any.whl (287 kB)\n",
      "Using cached google_cloud_recommendations_ai-0.10.8-py2.py3-none-any.whl (180 kB)\n",
      "Using cached google_cloud_spanner-3.42.0-py2.py3-none-any.whl (355 kB)\n",
      "Using cached google_cloud_storage-2.14.0-py2.py3-none-any.whl (121 kB)\n",
      "Using cached google_auth-2.28.0-py2.py3-none-any.whl (186 kB)\n",
      "Using cached google_cloud_videointelligence-2.13.1-py2.py3-none-any.whl (240 kB)\n",
      "Using cached google_cloud_vision-3.7.0-py2.py3-none-any.whl (460 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached jsonpickle-3.0.2-py3-none-any.whl (40 kB)\n",
      "Using cached objsize-0.7.0-py3-none-any.whl (11 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "Using cached tensorflow_serving_api-2.14.1-py2.py3-none-any.whl (26 kB)\n",
      "Using cached apache_beam-2.54.0-cp310-cp310-win_amd64.whl (5.0 MB)\n",
      "Using cached google_cloud_resource_manager-1.12.1-py2.py3-none-any.whl (333 kB)\n",
      "Using cached google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "Using cached grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Using cached grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: protobuf, packaging, objsize, keras, jsonpickle, httplib2, grpc-interceptor, google-crc32c, fasteners, fastavro, dill, proto-plus, oauth2client, js2py, hdfs, google-resumable-media, google-auth, tensorflow-metadata, grpcio-status, google-auth-httplib2, google-apitools, google-api-core, apache-beam, tensorboard, grpc-google-iam-v1, google-cloud-core, google-api-python-client, tensorflow-intel, google-cloud-vision, google-cloud-videointelligence, google-cloud-storage, google-cloud-spanner, google-cloud-resource-manager, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, tensorflow, google-cloud-pubsublite, google-cloud-aiplatform, tensorflow-serving-api, tfx-bsl, tensorflow-data-validation\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.10.0\n",
      "    Uninstalling keras-2.10.0:\n",
      "      Successfully uninstalled keras-2.10.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.15.0\n",
      "    Uninstalling google-auth-2.15.0:\n",
      "      Successfully uninstalled google-auth-2.15.0\n",
      "  Attempting uninstall: grpcio-status\n",
      "    Found existing installation: grpcio-status 1.54.2\n",
      "    Uninstalling grpcio-status-1.54.2:\n",
      "      Successfully uninstalled grpcio-status-1.54.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.3\n",
      "    Uninstalling tensorboard-2.12.3:\n",
      "      Successfully uninstalled tensorboard-2.12.3\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow-intel 2.12.0\n",
      "    Uninstalling tensorflow-intel-2.12.0:\n",
      "      Successfully uninstalled tensorflow-intel-2.12.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "Successfully installed apache-beam-2.54.0 dill-0.3.1.1 fastavro-1.9.4 fasteners-0.19 google-api-core-2.17.1 google-api-python-client-1.12.11 google-apitools-0.5.31 google-auth-2.28.0 google-auth-httplib2-0.1.1 google-cloud-aiplatform-1.42.1 google-cloud-bigquery-3.17.2 google-cloud-bigquery-storage-2.24.0 google-cloud-bigtable-2.23.0 google-cloud-core-2.4.1 google-cloud-datastore-2.19.0 google-cloud-dlp-3.15.1 google-cloud-language-2.13.1 google-cloud-pubsub-2.19.4 google-cloud-pubsublite-1.9.0 google-cloud-recommendations-ai-0.10.8 google-cloud-resource-manager-1.12.1 google-cloud-spanner-3.42.0 google-cloud-storage-2.14.0 google-cloud-videointelligence-2.13.1 google-cloud-vision-3.7.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 grpc-google-iam-v1-0.13.0 grpc-interceptor-0.15.4 grpcio-status-1.48.2 hdfs-2.7.3 httplib2-0.22.0 js2py-0.74 jsonpickle-3.0.2 keras-2.15.0 oauth2client-4.1.3 objsize-0.7.0 packaging-23.2 proto-plus-1.23.0 protobuf-3.20.3 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-data-validation-1.14.0 tensorflow-intel-2.15.0 tensorflow-metadata-1.14.0 tensorflow-serving-api-2.14.1 tfx-bsl-1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "multiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install ml_metadata\n",
    "# !pip install tensorflow-data-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05273bf2-c907-4b30-b420-c23fc1a06fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 21:33:13.777431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 21:33:16.863258: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-02-18 21:33:16.864656: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-02-18 21:33:16.864678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.11.0\n",
      "TFDV version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import tensorflow as tf\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "print('TFDV version: {}'.format(tfdv.version.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461dc8b-b8b1-493d-be5c-6b74703eb924",
   "metadata": {},
   "source": [
    "# Download dataset\n",
    "\n",
    "You will be using the Chicago Taxi dataset for this lab. Let's download the CSVs into your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0830f3-52dd-4794-b1d4-c3eedea91784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# location of the pipeline metadata store\n",
    "_pipeline_root = './pipeline/'\n",
    "\n",
    "# directory of the raw data files\n",
    "_data_root = './data/chicago_data'\n",
    "\n",
    "# path to the raw training data\n",
    "_data_filepath = os.path.join(_data_root, 'data.csv')\n",
    "\n",
    "## Download data\n",
    "os.makedirs(_data_root, exist_ok=True)\n",
    "if not os.path.isfile(_data_filepath):\n",
    "    #https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew\n",
    "    url =  url = 'https://docs.google.com/uc?export=download&id=1HFD4QZbbH3-Jbjv01LtfZVq7P3f2SgHJ'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(_data_filepath, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81237f8c-8620-4a8f-a20f-97b864f325db",
   "metadata": {},
   "source": [
    "The green box in the middle shows the data model followed by ML Metadata.\n",
    "\n",
    "https://www.tensorflow.org/tfx/guide/mlmd?hl=es-419#data_model\n",
    "\n",
    "![mlmd_overview](mlmd_overview.png)\n",
    "\n",
    "- **ArtifactType** describes an artifact's type and its properties that are stored in the metadata store. You can register these types on-the-fly with the metadata store in code, or you can load them in the store from a serialized format. Once you register a type, its definition is available throughout the lifetime of the store.\n",
    "- An **Artifact** describes a specific instance of an ArtifactType, and its properties that are written to the metadata store.\n",
    "- An **ExecutionType** describes a type of component or step in a workflow, and its runtime parameters.\n",
    "- An **Execution** is a record of a component run or a step in an ML workflow and the runtime parameters. An execution can be thought of as an instance of an ExecutionType. Executions are recorded when you run an ML pipeline or step.\n",
    "- An **Event** is a record of the relationship between artifacts and executions. When an execution happens, events record every artifact that was used by the execution, and every artifact that was produced. These records allow for lineage tracking throughout a workflow. By looking at all events, MLMD knows what executions happened and what artifacts were created as a result. MLMD can then recurse back from any artifact to all of its upstream inputs.\n",
    "- A **ContextType** describes a type of conceptual group of artifacts and executions in a workflow, and its structural properties. For example: projects, pipeline runs, experiments, owners etc.\n",
    "- A **Context** is an instance of a ContextType. It captures the shared information within the group. For example: project name, changelist commit id, experiment annotations etc. It has a user-defined unique name within its ContextType.\n",
    "- An **Attribution** is a record of the relationship between artifacts and contexts.\n",
    "- An **Association** is a record of the relationship between executions and contexts.\n",
    "\n",
    "\n",
    "You will use **TFDV** to generate a schema and record this process in the ML Metadata store. You will be starting from scratch so you will be defining each component of the data model. The outline of steps involve:\n",
    "\n",
    "1. Defining the ML Metadata's storage database\n",
    "2. Setting up the necessary artifact types\n",
    "3. Setting up the execution types\n",
    "4. Generating an input artifact unit\n",
    "5. Generating an execution unit\n",
    "6. Registering an input event\n",
    "7. Running the TFDV component\n",
    "8. Generating an output artifact unit\n",
    "9. Registering an output event\n",
    "10. Updating the execution unit\n",
    "11. Seting up and generating a context unit\n",
    "12. Generating attributions and associations\n",
    "\n",
    "![mlmd_overview](mlmd_flow.png)\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd/MetadataStore\n",
    "\n",
    "\n",
    "# Define ML Metadata's Storage Database\n",
    "\n",
    "\n",
    "The first step would be to instantiate your storage backend.There are several types supported such as fake (temporary) database, SQLite, MySQL, and even cloud-based storage. For this demo, you will just be using a fake database for quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c87a25a-3da2-4e1b-8d1f-d00303e594b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a connection config\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "\n",
    "# Set an empty fake database proto\n",
    "connection_config.fake_database.SetInParent() \n",
    "\n",
    "# Setup the metadata store\n",
    "store = metadata_store.MetadataStore(connection_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da7889-9e84-44e5-b655-bdc5ea003483",
   "metadata": {},
   "source": [
    "# Register ArtifactTypes\n",
    "\n",
    "Next, you will create the artifact types needed and register them to the store. Since our simple exercise will just involve generating a schema using TFDV, you will only create two artifact types: one for the input dataset and another for the output schema. The main steps will be to:\n",
    "\n",
    "- Declare an ArtifactType()\n",
    "- Define the name of the artifact type\n",
    "- Define the necessary properties within these artifact types. For example, it is important to know the data split name so you may want to have a split property for the artifact type that holds datasets.\n",
    "- Use put_artifact_type() to register them to the metadata store. This generates an id that you can use later to refer to a particular artifact type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97219a0-342f-446f-976a-1e9d75267638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data artifact type:\n",
      " name: \"DataSet\"\n",
      "properties {\n",
      "  key: \"name\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"split\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "\n",
      "Schema artifact type:\n",
      " name: \"Schema\"\n",
      "properties {\n",
      "  key: \"name\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "\n",
      "Data artifact type ID: 10\n",
      "Schema artifact type ID: 11\n"
     ]
    }
   ],
   "source": [
    "# Create ArtifactType for the input dataset\n",
    "data_artifact_type = metadata_store_pb2.ArtifactType()\n",
    "data_artifact_type.name = 'DataSet'\n",
    "data_artifact_type.properties['name'] = metadata_store_pb2.STRING\n",
    "data_artifact_type.properties['split'] = metadata_store_pb2.STRING\n",
    "data_artifact_type.properties['version'] = metadata_store_pb2.INT\n",
    "\n",
    "# Register artifact type to the Metadata Store\n",
    "data_artifact_type_id = store.put_artifact_type(data_artifact_type)\n",
    "\n",
    "# Create ArtifactType for Schema\n",
    "schema_artifact_type = metadata_store_pb2.ArtifactType()\n",
    "schema_artifact_type.name = 'Schema'\n",
    "schema_artifact_type.properties['name'] = metadata_store_pb2.STRING\n",
    "schema_artifact_type.properties['version'] = metadata_store_pb2.INT\n",
    "\n",
    "# Register artifact type to the Metadata Store\n",
    "schema_artifact_type_id = store.put_artifact_type(schema_artifact_type)\n",
    "\n",
    "print('Data artifact type:\\n', data_artifact_type)\n",
    "print('Schema artifact type:\\n', schema_artifact_type)\n",
    "print('Data artifact type ID:', data_artifact_type_id)\n",
    "print('Schema artifact type ID:', schema_artifact_type_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ef254-dfa7-4530-b206-8671fd9ca205",
   "metadata": {},
   "source": [
    "# Register ExecutionType\n",
    "\n",
    "You will then create the execution types needed. For the simple setup, you will just declare one for the data validation component with a state property so you can record if the process is running or already completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849620bd-3770-4a82-bcca-37dd887152a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation execution type:\n",
      " name: \"Data Validation\"\n",
      "properties {\n",
      "  key: \"state\"\n",
      "  value: STRING\n",
      "}\n",
      "\n",
      "Data validation execution type ID: 12\n"
     ]
    }
   ],
   "source": [
    "# Create ExecutionType for Data Validation component\n",
    "dv_execution_type = metadata_store_pb2.ExecutionType()\n",
    "dv_execution_type.name = 'Data Validation'\n",
    "dv_execution_type.properties['state'] = metadata_store_pb2.STRING\n",
    "\n",
    "# Register execution type to the Metadata Store\n",
    "dv_execution_type_id = store.put_execution_type(dv_execution_type)\n",
    "\n",
    "print('Data validation execution type:\\n', dv_execution_type)\n",
    "print('Data validation execution type ID:', dv_execution_type_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34065846-c173-4114-b0ce-9099eabf062f",
   "metadata": {},
   "source": [
    "# Generate input artifact unit\n",
    "\n",
    "With the artifact types created, you can now create instances of those types. The cell below creates the artifact for the input dataset. This artifact is recorded in the metadata store through the put_artifacts() function. Again, it generates an **id** that can be used for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ebe208-9f4f-4591-8cd9-4118f66c8495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data artifact:\n",
      " type_id: 10\n",
      "uri: \"./data/chicago_data/data.csv\"\n",
      "properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"Chicago Taxi dataset\"\n",
      "  }\n",
      "}\n",
      "properties {\n",
      "  key: \"split\"\n",
      "  value {\n",
      "    string_value: \"train\"\n",
      "  }\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "\n",
      "Data artifact ID: 1\n"
     ]
    }
   ],
   "source": [
    "# Declare input artifact of type DataSet\n",
    "data_artifact = metadata_store_pb2.Artifact()\n",
    "data_artifact.uri = _data_filepath\n",
    "data_artifact.type_id = data_artifact_type_id\n",
    "data_artifact.properties['name'].string_value = 'Chicago Taxi dataset'\n",
    "data_artifact.properties['split'].string_value = 'train'\n",
    "data_artifact.properties['version'].int_value = 1\n",
    "\n",
    "# Submit input artifact to the Metadata Store\n",
    "data_artifact_id = store.put_artifacts([data_artifact])[0]\n",
    "\n",
    "print('Data artifact:\\n', data_artifact)\n",
    "print('Data artifact ID:', data_artifact_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84381537-a7c8-4cda-8b67-418e43c2b727",
   "metadata": {},
   "source": [
    "# Generate execution unit\n",
    "\n",
    "Next, you will create an instance of the Data Validation execution type you registered earlier. You will set the state to **RUNNING** to signify that you are about to run the **TFDV** function. This is recorded with the **put_executions()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bbfed5-b5b5-4bc8-b213-14a09b55178c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation execution:\n",
      " type_id: 12\n",
      "properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"RUNNING\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Data validation execution ID: 1\n"
     ]
    }
   ],
   "source": [
    "# Register the Execution of a Data Validation run\n",
    "dv_execution = metadata_store_pb2.Execution()\n",
    "dv_execution.type_id = dv_execution_type_id\n",
    "dv_execution.properties['state'].string_value = 'RUNNING'\n",
    "\n",
    "# Submit execution unit to the Metadata Store\n",
    "dv_execution_id = store.put_executions([dv_execution])[0]\n",
    "\n",
    "print('Data validation execution:\\n', dv_execution)\n",
    "print('Data validation execution ID:', dv_execution_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8dda8c-588f-4631-a512-31ca0eec539d",
   "metadata": {},
   "source": [
    "# Register input event\n",
    "\n",
    "An event defines a relationship between artifacts and executions. You will generate the input event relationship for dataset artifact and data validation execution units. The list of event types are shown [here](https://github.com/google/ml-metadata/blob/master/ml_metadata/proto/metadata_store.proto#L187) and the event is recorded with the put_events() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b420468-6747-4859-8828-0313dd9bf19f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input event:\n",
      " artifact_id: 1\n",
      "execution_id: 1\n",
      "type: DECLARED_INPUT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Declare the input event\n",
    "input_event = metadata_store_pb2.Event()\n",
    "input_event.artifact_id = data_artifact_id\n",
    "input_event.execution_id = dv_execution_id\n",
    "input_event.type = metadata_store_pb2.Event.DECLARED_INPUT\n",
    "\n",
    "# Submit input event to the Metadata Store\n",
    "store.put_events([input_event])\n",
    "\n",
    "print('Input event:\\n', input_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8981997-3594-48d6-a163-8e8fffb493b1",
   "metadata": {},
   "source": [
    "# Run the TFDV component\n",
    "\n",
    "You will now run the TFDV component to generate the schema of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a1fea5-61bd-4db1-8651-d76625fba174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_data_validation/utils/statistics_io_impl.py:91: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_data_validation/utils/statistics_io_impl.py:91: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset's Schema has been generated at: ./schema.pbtxt\n"
     ]
    }
   ],
   "source": [
    "# Infer a schema by passing statistics to `infer_schema()`\n",
    "train_data = _data_filepath\n",
    "train_stats = tfdv.generate_statistics_from_csv(data_location=train_data)\n",
    "schema = tfdv.infer_schema(statistics=train_stats)\n",
    "\n",
    "schema_file = './schema.pbtxt'\n",
    "tfdv.write_schema_text(schema, schema_file)\n",
    "\n",
    "print(\"Dataset's Schema has been generated at:\", schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29884e-cfb8-4cdf-828e-38e9a2953404",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate output artifact unit\n",
    "\n",
    "Now that the TFDV component has finished running and schema has been generated, you can create the artifact for the generated schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624c5b95-2f9c-48b0-9ae8-5ab78963d27c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema artifact:\n",
      " type_id: 11\n",
      "uri: \"./schema.pbtxt\"\n",
      "properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"Chicago Taxi Schema\"\n",
      "  }\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "\n",
      "Schema artifact ID: 2\n"
     ]
    }
   ],
   "source": [
    "# Declare output artifact of type Schema_artifact\n",
    "schema_artifact = metadata_store_pb2.Artifact()\n",
    "schema_artifact.uri = schema_file\n",
    "schema_artifact.type_id = schema_artifact_type_id\n",
    "schema_artifact.properties['version'].int_value = 1\n",
    "schema_artifact.properties['name'].string_value = 'Chicago Taxi Schema'\n",
    "\n",
    "# Submit output artifact to the Metadata Store\n",
    "schema_artifact_id = store.put_artifacts([schema_artifact])[0]\n",
    "\n",
    "print('Schema artifact:\\n', schema_artifact)\n",
    "print('Schema artifact ID:', schema_artifact_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ce737-d319-4e76-874b-93cbb4096d2d",
   "metadata": {},
   "source": [
    "# Register output event\n",
    "\n",
    "Analogous to the input event earlier, you also want to define an output event to record the ouput artifact of a particular execution unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ed763b-1256-4b07-8e20-a8946fb71d72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output event:\n",
      " artifact_id: 2\n",
      "execution_id: 1\n",
      "type: DECLARED_OUTPUT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Declare the output event\n",
    "output_event = metadata_store_pb2.Event()\n",
    "output_event.artifact_id = schema_artifact_id\n",
    "output_event.execution_id = dv_execution_id\n",
    "output_event.type = metadata_store_pb2.Event.DECLARED_OUTPUT\n",
    "\n",
    "# Submit output event to the Metadata Store\n",
    "store.put_events([output_event])\n",
    "\n",
    "print('Output event:\\n', output_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e3f79-cb73-4bdd-8ca6-6ffc6f2dc3e4",
   "metadata": {},
   "source": [
    "# Update the execution unit\n",
    "\n",
    "As the TFDV component has finished running successfully, you need to update the state of the execution unit and record it again to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac641620-c322-42fa-adec-3cc04d138a51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation execution:\n",
      " id: 1\n",
      "type_id: 12\n",
      "properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"COMPLETED\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mark the `state` as `COMPLETED`\n",
    "dv_execution.id = dv_execution_id\n",
    "dv_execution.properties['state'].string_value = 'COMPLETED'\n",
    "\n",
    "# Update execution unit in the Metadata Store\n",
    "store.put_executions([dv_execution])\n",
    "\n",
    "print('Data validation execution:\\n', dv_execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224377cc-338c-4001-ba5f-53ee908979dd",
   "metadata": {},
   "source": [
    "# Setting up Context Types and Generating a Context Unit\n",
    "\n",
    "You can group the artifacts and execution units into a **Context**. First, you need to define a **ContextType** which defines the required context. It follows a similar format as artifact and event types. You can register this with the **put_context_type()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36354290-c6e7-457e-8557-45bffc7911ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a ContextType\n",
    "expt_context_type = metadata_store_pb2.ContextType()\n",
    "expt_context_type.name = 'Experiment'\n",
    "expt_context_type.properties['note'] = metadata_store_pb2.STRING\n",
    "\n",
    "# Register context type to the Metadata Store\n",
    "expt_context_type_id = store.put_context_type(expt_context_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2e371-dc56-42c9-b38f-04e79892461a",
   "metadata": {},
   "source": [
    "Similarly, you can create an instance of this context type and use the put_contexts() method to register to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f308f9-378c-44e2-a4ff-5c835bb8ec43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Context type:\n",
      " name: \"Experiment\"\n",
      "properties {\n",
      "  key: \"note\"\n",
      "  value: STRING\n",
      "}\n",
      "\n",
      "Experiment Context type ID:  13\n",
      "Experiment Context:\n",
      " type_id: 13\n",
      "name: \"Demo\"\n",
      "properties {\n",
      "  key: \"note\"\n",
      "  value {\n",
      "    string_value: \"Walkthrough of metadata\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Experiment Context ID:  1\n"
     ]
    }
   ],
   "source": [
    "# Generate the context\n",
    "expt_context = metadata_store_pb2.Context()\n",
    "expt_context.type_id = expt_context_type_id\n",
    "# Give the experiment a name\n",
    "expt_context.name = 'Demo'\n",
    "expt_context.properties['note'].string_value = 'Walkthrough of metadata'\n",
    "\n",
    "# Submit context to the Metadata Store\n",
    "expt_context_id = store.put_contexts([expt_context])[0]\n",
    "\n",
    "print('Experiment Context type:\\n', expt_context_type)\n",
    "print('Experiment Context type ID: ', expt_context_type_id)\n",
    "\n",
    "print('Experiment Context:\\n', expt_context)\n",
    "print('Experiment Context ID: ', expt_context_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d73ab1a-aecd-46b4-8133-04510688307f",
   "metadata": {},
   "source": [
    "# Generate attribution and association relationships\n",
    "\n",
    "With the **Context** defined, you can now create its relationship with the artifact and executions you previously used. You will create the relationship between schema artifact unit and experiment context unit to form an **Attribution**. Similarly, you will create the relationship between data validation execution unit and experiment context unit to form an **Association**. These are registered with the **put_attributions_and_associations()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e2f063a-b836-42ab-9927-ddfbe0a8946a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Attribution:\n",
      " artifact_id: 2\n",
      "context_id: 1\n",
      "\n",
      "Experiment Association:\n",
      " execution_id: 1\n",
      "context_id: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the attribution\n",
    "expt_attribution = metadata_store_pb2.Attribution()\n",
    "expt_attribution.artifact_id = schema_artifact_id\n",
    "expt_attribution.context_id = expt_context_id\n",
    "\n",
    "# Generate the association\n",
    "expt_association = metadata_store_pb2.Association()\n",
    "expt_association.execution_id = dv_execution_id\n",
    "expt_association.context_id = expt_context_id\n",
    "\n",
    "# Submit attribution and association to the Metadata Store\n",
    "store.put_attributions_and_associations([expt_attribution], [expt_association])\n",
    "\n",
    "print('Experiment Attribution:\\n', expt_attribution)\n",
    "print('Experiment Association:\\n', expt_association)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39047f85-e08a-408b-8528-f790e9c54a1b",
   "metadata": {},
   "source": [
    "# Retrieving Information from the Metadata Store\n",
    "\n",
    "You've now recorded the needed information to the metadata store. If we did this in a persistent database, you can track which artifacts and events are related to each other even without seeing the code used to generate it. See a sample run below where you investigate what dataset is used to generate the schema. (*It would be obvious which dataset is used in our simple demo because we only have two artifacts registered. Thus, assume that you have thousands of entries in the metadata store.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b52d07-d8a2-40f0-b0c3-213ed343076b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: 10\n",
       " name: \"DataSet\"\n",
       " properties {\n",
       "   key: \"name\"\n",
       "   value: STRING\n",
       " }\n",
       " properties {\n",
       "   key: \"split\"\n",
       "   value: STRING\n",
       " }\n",
       " properties {\n",
       "   key: \"version\"\n",
       "   value: INT\n",
       " },\n",
       " id: 11\n",
       " name: \"Schema\"\n",
       " properties {\n",
       "   key: \"name\"\n",
       "   value: STRING\n",
       " }\n",
       " properties {\n",
       "   key: \"version\"\n",
       "   value: INT\n",
       " }]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get artifact types\n",
    "store.get_artifact_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01cd6083-5733-4743-ab15-e7a78cf3ca46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 2\n",
      "type_id: 11\n",
      "uri: \"./schema.pbtxt\"\n",
      "properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"Chicago Taxi Schema\"\n",
      "  }\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "create_time_since_epoch: 1708292043141\n",
      "last_update_time_since_epoch: 1708292043141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get 1st element in the list of `Schema` artifacts.\n",
    "# You will investigate which dataset was used to generate it.\n",
    "schema_to_inv = store.get_artifacts_by_type('Schema')[0]\n",
    "\n",
    "# print output\n",
    "print(schema_to_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b383b4-b64a-4844-9306-81fe50e01cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[artifact_id: 2\n",
      "execution_id: 1\n",
      "type: DECLARED_OUTPUT\n",
      "milliseconds_since_epoch: 1708292043160\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Get events related to the schema id\n",
    "schema_events = store.get_events_by_artifact_ids([schema_to_inv.id])\n",
    "\n",
    "print(schema_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ab0f8-c395-4660-9ce8-c3c8b5126baf",
   "metadata": {},
   "source": [
    "You see that it is an output of an execution so you can look up the execution id to see related artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1af85fd-535f-429c-911b-aa13d6cc05fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[artifact_id: 1\n",
      "execution_id: 1\n",
      "type: DECLARED_INPUT\n",
      "milliseconds_since_epoch: 1708292036530\n",
      ", artifact_id: 2\n",
      "execution_id: 1\n",
      "type: DECLARED_OUTPUT\n",
      "milliseconds_since_epoch: 1708292043160\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Get events related to the output above\n",
    "execution_events = store.get_events_by_execution_ids([schema_events[0].execution_id])\n",
    "\n",
    "print(execution_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a094964d-6dc7-4edd-85bc-8aaa69933eb7",
   "metadata": {},
   "source": [
    "You see the declared input of this execution so you can select that from the list and lookup the details of the artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abf26f53-5c8e-42d7-bc16-32f98171d7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: 1\n",
       " type_id: 10\n",
       " uri: \"./data/chicago_data/data.csv\"\n",
       " properties {\n",
       "   key: \"name\"\n",
       "   value {\n",
       "     string_value: \"Chicago Taxi dataset\"\n",
       "   }\n",
       " }\n",
       " properties {\n",
       "   key: \"split\"\n",
       "   value {\n",
       "     string_value: \"train\"\n",
       "   }\n",
       " }\n",
       " properties {\n",
       "   key: \"version\"\n",
       "   value {\n",
       "     int_value: 1\n",
       "   }\n",
       " }\n",
       " create_time_since_epoch: 1708292035104\n",
       " last_update_time_since_epoch: 1708292035104]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up the artifact that is a declared input\n",
    "artifact_input = execution_events[0]\n",
    "\n",
    "store.get_artifacts_by_id([artifact_input.artifact_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
